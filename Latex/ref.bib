
@misc{NIND,
    author = {{National Institute of Neurological Disorders and Stroke}},
    year = {2016},
    title = {Parkinson's Disease Information Page},
    howpublished = {\url{https://www.ninds.nih.gov/Disorders/All-Disorders/Parkinsons-Disease-Information-Page#disorders-r3}}    
}

@article{jankovic2008parkinson,
  title={Parkinson’s disease: clinical features and diagnosis},
  author={Jankovic, Joseph},
  journal={Journal of neurology, neurosurgery \& psychiatry},
  volume={79},
  number={4},
  pages={368--376},
  year={2008},
  publisher={BMJ Publishing Group Ltd}
}


@misc{Symptoms,
    author = {{Jonathan Smith}},
    year = {2018},
    title = {Parkinson’s Disease Gene Therapy Clinical Trial Launched in UK},
    howpublished = {\url{https://www.labiotech.eu/trends-news/axovant-parkinsons-disease-gene}}    
}


@misc{WORKING,
    author = {Shawn},
    year = {2020},
    title = {What is IMU Sensor and How to use with Arduino?},
    howpublished = {\url{https://www.seeedstudio.com/blog/2020/01/17/what-is-imu-sensor-overview-with-arduino-usage-guide/}}    
}



@misc{IMU,
    author = {{MathWorks}},
    title = {Model IMU, GPS, and INS/GPS},
    howpublished = {\url{https://de.mathworks.com/help/fusion/gs/model-imu-gps-and-insgps.html}}    
}

@misc{ShimmerSensor ,
    author = {{Shimmer Research}},
    title = {Shimmer Sensor},
    howpublished = {\url{https://shimmersensing.com}}    
}


@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude Elwood},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}


@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC and Mac Kay, David JC and others},
  year={2003},
  publisher={Cambridge university press}
}

@book{cover1999elements,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}


@article{MonteCarlo,
  title={Taboga, Marco (2021). "The Monte Carlo method", Lectures on probability theory and mathematical statistics. Kindle Direct Publishing. Online appendix. https://www.statlect.com/asymptotic-theory/Monte-Carlo-method.}
}



@misc{MJFF,
    author = {{MJFF Levodopa Response Study}},
    year = {2019},
    title = {MJFF Levodopa Wearable Sensors Dataset},
    howpublished = {\url{https://www.synapse.org/#!Synapse:syn20681023/wiki/594678}}    
}

@article{anderson1986scientific,
  title={Scientific uses of the MANIAC},
  author={Anderson, Herbert L},
  journal={Journal of Statistical Physics},
  volume={43},
  number={5},
  pages={731--748},
  year={1986},
  publisher={Springer}
}

@article{binder1993monte,
  title={The Monte Carlo method in condensed matter physics},
  author={Binder, Kurt and Girvin, Steven M},
  journal={Physics Today},
  volume={46},
  number={6},
  pages={94},
  year={1993}
}


@incollection{PATHRIA201139,
title = {3 - The Canonical Ensemble},
editor = {R.K. Pathria and Paul D. Beale},
booktitle = {Statistical Mechanics (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {39-90},
year = {2011},
isbn = {978-0-12-382188-1},
doi = {https://doi.org/10.1016/B978-0-12-382188-1.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123821881000037},
author = {R.K. Pathria and Paul D. Beale}
}



@misc{zheng2016estimating,
      title={Estimating mutual information in high dimensions via classification error}, 
      author={Charles Y. Zheng and Yuval Benjamini},
      year={2016},
      eprint={1606.05229},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{kraskov2004estimating,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical review E},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}


@misc{gao2018estimating,
      title={Estimating Mutual Information for Discrete-Continuous Mixtures}, 
      author={Weihao Gao and Sreeram Kannan and Sewoong Oh and Pramod Viswanath},
      year={2018},
      eprint={1709.06212},
      archivePrefix={arXiv},
      primaryClass={cs.IT}
}


@article{iosa2016wearable,
  title={Wearable inertial sensors for human movement analysis},
  author={Iosa, Marco and Picerno, Pietro and Paolucci, Stefano and Morone, Giovanni},
  journal={Expert review of medical devices},
  volume={13},
  number={7},
  pages={641--659},
  year={2016},
  publisher={Taylor \& Francis}
}


@article{gao2017estimating,
  title={Estimating mutual information for discrete-continuous mixtures},
  author={Gao, Weihao and Kannan, Sreeram and Oh, Sewoong and Viswanath, Pramod},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}






@article{carrara2020estimation,
  title={On the estimation of mutual information},
  author={Carrara, Nicholas and Ernst, Jesse},
  journal={Multidisciplinary Digital Publishing Institute Proceedings},
  volume={33},
  number={1},
  pages={31},
  year={2020}
}

@ARTICLE{1057418,
  author={Kreer, J.},
  journal={IRE Transactions on Information Theory}, 
  title={A question of terminology}, 
  year={1957},
  volume={3},
  number={3},
  pages={208-208},
  doi={10.1109/TIT.1957.1057418}}

@article{srinivasa2005review,
  title={A review on multivariate mutual information},
  author={Srinivasa, Sunil},
  journal={Univ. of Notre Dame, Notre Dame, Indiana},
  volume={2},
  number={1},
  year={2005}
}



 @misc{Klinik,
    author = {{Tremor}},
    year={2020},
    title = {Ursachen & Symptome},
    howpublished = {\url{https://www.schoen-klinik.de/tremor}}    
}



@article{altman1992introduction,
  title={An introduction to kernel and nearest-neighbor nonparametric regression},
  author={Altman, Naomi S},
  journal={The American Statistician},
  volume={46},
  number={3},
  pages={175--185},
  year={1992},
  publisher={Taylor \& Francis}
}

@article{peterson2009k,
  title={K-nearest neighbor},
  author={Peterson, Leif E},
  journal={Scholarpedia},
  volume={4},
  number={2},
  pages={1883},
  year={2009}
}


@TECHREPORT{Omohundro89fiveballtree,
    author = {Stephen M. Omohundro},
    title = {Five Balltree Construction Algorithms},
    institution = {},
    year = {1989}
}


@article{10.1145/361002.361007, author = {Bentley, Jon Louis}, title = {Multidimensional Binary Search Trees Used for Associative Searching}, year = {1975}, issue_date = {Sept. 1975}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, volume = {18}, number = {9}, issn = {0001-0782}, url = {https://doi.org/10.1145/361002.361007}, doi = {10.1145/361002.361007}, abstract = {This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O(log n); deletion of the root, O(n(k-1)/k); deletion of a random node, O(log n); and optimization (guarantees logarithmic performance of searches), O(n log n). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O(n(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O(log n).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given.}, journal = {Commun. ACM}, month = {sep}, pages = {509–517}, numpages = {9}, keywords = {partial match queries, nearest neighbor queries, information retrieval system, binary tree insertion, intersection queries, key, attribute, binary search trees, associative retrieval} }


@article{singh2013k,
  title={K-means with Three different Distance Metrics},
  author={Singh, Archana and Yadav, Avantika and Rana, Ajay},
  journal={International Journal of Computer Applications},
  volume={67},
  number={10},
  year={2013},
  publisher={Citeseer}
}


@article{zhang2017learning,
  title={Learning k for knn classification},
  author={Zhang, Shichao and Li, Xuelong and Zong, Ming and Zhu, Xiaofeng and Cheng, Debo},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={8},
  number={3},
  pages={1--19},
  year={2017},
  publisher={ACM New York, NY, USA}
}


@inproceedings{guo2003knn,
  title={KNN model-based approach in classification},
  author={Guo, Gongde and Wang, Hui and Bell, David and Bi, Yaxin and Greer, Kieran},
  booktitle={OTM Confederated International Conferences" On the Move to Meaningful Internet Systems"},
  pages={986--996},
  year={2003},
  organization={Springer}
}

@book{Lihang2012Statisticalearning methods,
  title={Statistical learning methods},
  author={Li hang and others},
  year={2012},
  publisher={Qing hua da xue chu ban she}
}



@article{luengo2020survey,
  title={A survey of Monte Carlo methods for parameter estimation},
  author={Luengo, David and Martino, Luca and Bugallo, M{\'o}nica and Elvira, V{\'\i}ctor and S{\"a}rkk{\"a}, Simo},
  journal={EURASIP Journal on Advances in Signal Processing},
  volume={2020},
  number={1},
  pages={1--62},
  year={2020},
  publisher={SpringerOpen}
}


@article{luengo2020survey,
  title={A survey of Monte Carlo methods for parameter estimation},
  author={Luengo, David and Martino, Luca and Bugallo, M{\'o}nica and Elvira, V{\'\i}ctor and S{\"a}rkk{\"a}, Simo},
  journal={EURASIP Journal on Advances in Signal Processing},
  volume={2020},
  number={1},
  pages={1--62},
  year={2020},
  publisher={SpringerOpen}
}


@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}


@article{RN3,
   author = {Ron, Dana},
   title = {Guest Editor's Introduction},
   journal = {Machine Learning},
   volume = {30},
   number = {1},
   pages = {5-6},
   ISSN = {1573-0565},
   DOI = {10.1023/A:1007411609915},
   url = {https://doi.org/10.1023/A:1007411609915},
   year = {1998},
   type = {Journal Article}
}

@article{RN4,
   author = {Xu, Yun and Goodacre, Royston},
   title = {On Splitting Training and Validation Set: A Comparative Study of Cross-Validation, Bootstrap and Systematic Sampling for Estimating the Generalization Performance of Supervised Learning},
   journal = {Journal of Analysis and Testing},
   volume = {2},
   number = {3},
   pages = {249-262},
   abstract = {Model validation is the most important part of building a supervised model. For building a model with good generalization performance one must have a sensible data splitting strategy, and this is crucial for model validation. In this study, we conducted a comparative study on various reported data splitting methods. The MixSim model was employed to generate nine simulated datasets with different probabilities of mis-classification and variable sample sizes. Then partial least squares for discriminant analysis and support vector machines for classification were applied to these datasets. Data splitting methods tested included variants of cross-validation, bootstrapping, bootstrapped Latin partition, Kennard-Stone algorithm (K-S) and sample set partitioning based on joint X–Y distances algorithm (SPXY). These methods were employed to split the data into training and validation sets. The estimated generalization performances from the validation sets were then compared with the ones obtained from the blind test sets which were generated from the same distribution but were unseen by the training/validation procedure used in model construction. The results showed that the size of the data is the deciding factor for the qualities of the generalization performance estimated from the validation set. We found that there was a significant gap between the performance estimated from the validation set and the one from the test set for the all the data splitting methods employed on small datasets. Such disparity decreased when more samples were available for training/validation, and this is because the models were then moving towards approximations of the central limit theory for the simulated datasets used. We also found that having too many or too few samples in the training set had a negative effect on the estimated model performance, suggesting that it is necessary to have a good balance between the sizes of training set and validation set to have a reliable estimation of model performance. We also found that systematic sampling method such as K-S and SPXY generally had very poor estimation of the model performance, most likely due to the fact that they are designed to take the most representative samples first and thus left a rather poorly representative sample set for model performance estimation.},
   ISSN = {2509-4696},
   DOI = {10.1007/s41664-018-0068-2},
   url = {https://doi.org/10.1007/s41664-018-0068-2},
   year = {2018},
   type = {Journal Article}
}


@book{harrington2012machine,
  title={Machine learning in action},
  author={Harrington, Peter},
  year={2012},
  publisher={Simon and Schuster}
}

@INPROCEEDINGS{8079967,  author={Abu Amra, Ihsan A. and Maghari, Ashraf Y. A.},  booktitle={2017 8th International Conference on Information Technology (ICIT)},   title={Students performance prediction using KNN and Naïve Bayesian},   year={2017},  volume={},  number={},  pages={909-913},  doi={10.1109/ICITECH.2017.8079967}}




@incollection{earl2008monte,
  title={Monte carlo simulations},
  author={Earl, David J and Deem, Michael W},
  booktitle={Molecular modeling of proteins},
  pages={25--36},
  year={2008},
  publisher={Springer}
}


@inproceedings{raychaudhuri2008introduction,
  title={Introduction to monte carlo simulation},
  author={Raychaudhuri, Samik},
  booktitle={2008 Winter simulation conference},
  pages={91--100},
  year={2008},
  organization={IEEE}
}

@article{steinbach2009knn,
  title={kNN: k-nearest neighbors},
  author={Steinbach, Michael and Tan, Pang-Ning},
  journal={The top ten algorithms in data mining},
  pages={151--162},
  year={2009},
  publisher={Chapman and Hall/CRC Boca Raton, FL}
}


@article{deng2016efficient,
  title={Efficient kNN classification algorithm for big data},
  author={Deng, Zhenyun and Zhu, Xiaoshu and Cheng, Debo and Zong, Ming and Zhang, Shichao},
  journal={Neurocomputing},
  volume={195},
  pages={143--148},
  year={2016},
  publisher={Elsevier}
}






@article{SAMII20041783,
title = {Parkinson's disease},
journal = {The Lancet},
volume = {363},
number = {9423},
pages = {1783-1793},
year = {2004},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(04)16305-8},
url = {https://www.sciencedirect.com/science/article/pii/S0140673604163058},
author = {Ali Samii and John G Nutt and Bruce R Ransom},
abstract = {Summary
Parkinson's disease is the most common serious movement disorder in the world, affecting about 1% of adults older than 60 years. The disease is attributed to selective loss of neurons in the substantia nigra, and its cause is enigmatic in most individuals. Symptoms of Parkinson's disease respond in varying degrees to drugs, and surgery offers hope for patients no longer adequately controlled in this manner. The high prevalence of the disease, and important advances in its management, mean that generalists need to have a working knowledge of this disorder. This Seminar covers the basics, from terminology to aspects of diagnosis, treatment, and pathogenesis.}
}


@article{iosa2016wearable,
  title={Wearable inertial sensors for human movement analysis},
  author={Iosa, Marco and Picerno, Pietro and Paolucci, Stefano and Morone, Giovanni},
  journal={Expert review of medical devices},
  volume={13},
  number={7},
  pages={641--659},
  year={2016},
  publisher={Taylor \& Francis}
}





@misc{KNNK,
    author = {{Amey Band}},
    year = {2020},
    title = {How to find the optimal value of K in KNN?},
    howpublished = {\url{https://towardsdatascience.com/how-to-find-the-optimal-value-of-k-in-knn-35d936e554eb}}    
}

@misc{KKK,
    author = {{Avinash Navlani}},
    year = {2018},
    title = {KNN Classification Tutorial using Scikit-learn},
    howpublished = {\url{https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn}}    
}

@book{landau2011cluster,
  title={Cluster analysis},
  author={Landau, Sabine and Leese, Morven and Stahl, Daniel and Everitt, Brian S},
  year={2011},
  publisher={John Wiley \& Sons}
}

@article{hall2008choice,
  title={Choice of neighbor order in nearest-neighbor classification},
  author={Hall, Peter and Park, Byeong U and Samworth, Richard J},
  journal={the Annals of Statistics},
  volume={36},
  number={5},
  pages={2135--2152},
  year={2008},
  publisher={Institute of Mathematical Statistics}
}




@article{2020,
   title={Quantifying the loss of information from binning list-mode data},
   volume={37},
   ISSN={1520-8532},
   url={http://dx.doi.org/10.1364/JOSAA.375317},
   DOI={10.1364/josaa.375317},
   number={3},
   journal={Journal of the Optical Society of America A},
   publisher={The Optical Society},
   author={Clarkson, Eric and Kupinski, Meredith},
   year={2020},
   month={Feb},
   pages={450} }


@article{2019,
   title={Quantifying information loss on chaotic attractors through recurrence networks},
   volume={383},
   ISSN={0375-9601},
   url={http://dx.doi.org/10.1016/j.physleta.2019.125854},
   DOI={10.1016/j.physleta.2019.125854},
   number={27},
   journal={Physics Letters A},
   publisher={Elsevier BV},
   author={Harikrishnan, K.P. and Misra, R. and Ambika, G.},
   year={2019},
   month={Sep},
   pages={125854} }

@article{hazry2009study,
  title={Study of inertial measurement unit sensor},
  author={Hazry, Desa and Sofian, Mohd and Rosbi, Mohammad},
  year={2009},
  publisher={Universiti Malaysia Perlis}
}


@article{porta2012k,
  title={K-nearest-neighbor conditional entropy approach for the assessment of the short-term complexity of cardiovascular control},
  author={Porta, A and Castiglioni, P and Bari, Vlasta and Bassani, T and Marchi, A and Cividjian, A and Quintin, L and Di Rienzo, M},
  journal={Physiological measurement},
  volume={34},
  number={1},
  pages={17},
  year={2012},
  publisher={IOP Publishing}
}

@article{steinbach2009knn,
  title={kNN: k-nearest neighbors},
  author={Steinbach, Michael and Tan, Pang-Ning},
  journal={The top ten algorithms in data mining},
  pages={151--162},
  year={2009},
  publisher={Chapman and Hall/CRC Boca Raton, FL}
}

@article{kozma2008k,
  title={k Nearest Neighbors algorithm (kNN)},
  author={Kozma, Laszlo},
  journal={Helsinki University of Technology},
  volume={32},
  year={2008}
}